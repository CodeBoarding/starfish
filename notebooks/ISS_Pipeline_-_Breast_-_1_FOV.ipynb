{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce In-situ Sequencing results with Starfish\n",
    "\n",
    "This notebook walks through a work flow that reproduces an ISS result for one field of view using the starfish package.\n",
    "\n",
    "## Load tiff stack and visualize one field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from showit import image\n",
    "import pprint\n",
    "\n",
    "from starfish import data, FieldOfView, IntensityTable\n",
    "from starfish.types import Features, Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_data = os.getenv(\"USE_TEST_DATA\") is not None\n",
    "experiment = data.ISS(use_test_data=use_test_data)\n",
    "\n",
    "\n",
    "# s.image.squeeze() simply converts the 4D tensor H*C*X*Y into a list of len(H*C) image planes for rendering by 'tile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show input file format that specifies how the tiff stack is organized\n",
    "\n",
    "The stack contains multiple single plane images, one for each color channel, 'c', (columns in above image) and imaging round, 'r', (rows in above image). This protocol assumes that genes are encoded with a length 4 quatenary barcode that can be read out from the images. Each round encodes a position in the codeword. The maximum signal in each color channel (columns in the above image) corresponds to a letter in the codeword. The channels, in order, correspond to the letters: 'T', 'G', 'C', 'A'. The goal is now to process these image data into spatially organized barcodes, e.g., ACTG, which can then be mapped back to a codebook that specifies what gene this codeword corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(experiment._src_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flat TIFF files are loaded into a 4-d tensor with dimensions corresponding to imaging round, channel, x, and y. For other volumetric approaches that image the z-plane, this would be a 5-d tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov = experiment.fov()\n",
    "all_intensities = list()\n",
    "for primary_image, dots, nuclei in zip(fov.iterate_image_type(FieldOfView.PRIMARY_IMAGES), fov.iterate_image_type('dots'), fov.iterate_image_type(\"nuclei\")):\n",
    "    images = [primary_image, nuclei, dots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Encoder table is the hypothesized standardized file format for the output of a spot detector, and is the first output file format in the pipeline that is not an image or set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attributes` is produced by the encoder and contains all the information necessary to map the encoded spots back to the original image\n",
    "\n",
    "`x, y` describe the position, while `x_min` through `y_max` describe the bounding box for the spot, which is refined by a radius `r`. This table also stores the intensity and spot_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each assay type also exposes a decoder. A decoder translates each spot (spot_id) in the Encoder table into a gene (that matches a barcode) and associates this information with the stored position. The goal is to decode and output a quality score that describes the confidence in the decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are hard and soft decodings -- hard decoding is just looking for the max value in the code book. Soft decoding, by contrast, finds the closest code by distance (in intensity). Because different assays each have their own intensities and error modes, we leave decoders as user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = IntensityTable.concatanate_intensity_tables(all_intensities)\n",
    "decoded = experiment.codebook.decode_per_round_max(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to results from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides house keeping genes, VIM and HER2 should be most highly expessed, which is consistent here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes, counts = np.unique(decoded.loc[decoded[Features.PASSES_THRESHOLDS]][Features.TARGET], return_counts=True)\n",
    "table = pd.Series(counts, index=genes).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling spots and decoding their gene information, cells must be segmented to assign genes to cells. This paper used a seeded watershed approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.image import Segmentation\n",
    "\n",
    "dapi_thresh = .16  # binary mask for cell (nuclear) locations\n",
    "stain_thresh = .22  # binary mask for overall cells // binarization of stain\n",
    "min_dist = 57\n",
    "\n",
    "registered_mp = registered_image.max_proj(Axes.CH, Axes.ZPLANE)\n",
    "registered_mp_numpy = registered_mp._squeezed_numpy(Axes.CH, Axes.ZPLANE)\n",
    "stain = np.mean(registered_mp_numpy, axis=0)\n",
    "stain = stain/stain.max()\n",
    "nuclei = nuclei.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "nuclei_numpy = nuclei._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "\n",
    "seg = Segmentation.Watershed(\n",
    "    nuclei_threshold=dapi_thresh,\n",
    "    input_threshold=stain_thresh,\n",
    "    min_distance=min_dist\n",
    ")\n",
    "label_image = seg.run(registered_image, nuclei)\n",
    "seg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "This FOV was selected to make sure that we can visualize the tumor/stroma boundary, below this is described by pseudo-coloring `HER2` (tumor) and vimentin (`VIM`, stroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "GENE1 = 'HER2'\n",
    "GENE2 = 'VIM'\n",
    "\n",
    "rgb = np.zeros(registered_image.tile_shape + (3,))\n",
    "nuclei_mp = nuclei.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "nuclei_numpy = nuclei_mp._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "rgb[:,:,0] = nuclei_numpy\n",
    "dots_mp = dots.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "dots_mp_numpy = dots_mp._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "rgb[:,:,1] = dots_mp_numpy\n",
    "do = rgb2gray(rgb)\n",
    "do = do/(do.max())\n",
    "\n",
    "image(do,size=10)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    is_gene1 = decoded.where(decoded[Features.AXIS][Features.TARGET] == GENE1, drop=True)\n",
    "    is_gene2 = decoded.where(decoded[Features.AXIS][Features.TARGET] == GENE2, drop=True)\n",
    "\n",
    "plt.plot(is_gene1.x, is_gene1.y, 'or')\n",
    "plt.plot(is_gene2.x, is_gene2.y, 'ob')\n",
    "plt.title(f'Red: {GENE1}, Blue: {GENE2}');"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}